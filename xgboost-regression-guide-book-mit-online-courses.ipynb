{"cells":[{"metadata":{},"cell_type":"markdown","source":"# XGBoost is one of the most popular machine learning algorithm these days. Regardless of the type of prediction task at hand; regression or classification.\n## I will predict the students that have certified of > 50% Course by applying XGBoost algorithm!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# In this noote book I will explain the following:\n## 1- Data visualization\n## 2- Data prepration and dummy pandas function\n## 3- XGBoost and its hyperprameters\n## 4- XGBoost tree visualization\n## 5- XGBoost features importance visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing libraries & Data:","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_MIT= pd.read_csv('../input/course-study/appendix.csv')\ndf_MIT.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_MIT.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_MIT=df_MIT.drop(['% Certified','Course Number','Course Title','% Grade Higher Than Zero'],axis=1)\ndf_MIT","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_MIT['Course Subject'].value_counts() #DUMMIES","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_MIT['Instructors'].value_counts() #DELETE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### View nulls values in data set by heatmap:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(df_MIT.isnull(),cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A- Let's do Quick Data visualization!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"figure= plt.figure(figsize=(10,10))\nsns.heatmap(df_MIT.corr(), annot=True,cmap=\"YlGnBu\")\n#To show the correlation between variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure= plt.figure(figsize=(20,10))\nsns.boxenplot(x='Course Subject',y='% Certified of > 50% Course Content Accessed',data=df_MIT,palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure= plt.figure(figsize=(20,10))\nsns.boxenplot('Participants (Course Content Accessed)','Institution',data=df_MIT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pairplot_cols=df_MIT[['Institution','Audited (> 50% Course Content Accessed)','Year','% Certified of > 50% Course Content Accessed','Total Course Hours (Thousands)','% Female','% Male','Median Age']]\nplt.figure(figsize=(20,20))\nsns.pairplot(df_pairplot_cols,hue='Institution',palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='Participants (Course Content Accessed)',y='Audited (> 50% Course Content Accessed)',data=df_MIT,col='Course Subject',hue='Institution',palette='coolwarm',\n          aspect=0.6,size=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='Median Age',y='% Female',data=df_MIT,col='Course Subject',hue='Institution',palette='coolwarm',\n          aspect=0.6,size=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='Median Age',y='% Male',data=df_MIT,col='Course Subject',hue='Institution',palette='coolwarm',\n          aspect=0.6,size=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= df_MIT['Median Hours for Certification']\ny= df_MIT['% Certified of > 50% Course Content Accessed']\ncmap = sns.cubehelix_palette(light=1, as_cmap=True)\nplt.figure(figsize=(10,10))\nsns.kdeplot(x, y, cmap=cmap, shade=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= df_MIT['Median Age']\ny= df_MIT['% Certified of > 50% Course Content Accessed']\nplt.figure(figsize=(10,10))\nsns.kdeplot(x, y, shade=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# B- Perperation of data:\n## I will drop : 'Total Course Hours (Thousands)' , 'Certified' , 'Audited (> 50% Course Content Accessed)' , 'Instructors' , 'Launch Date' , '% Played Video'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_MIT.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_XGB = df_MIT.drop(['Total Course Hours (Thousands)','Certified','Audited (> 50% Course Content Accessed)','Instructors','Launch Date','% Played Video'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_XGB.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pandas get_dummies() :\n### Pandas has a function which can turn a categorical variable into a series of zeros and ones, which makes them a lot easier to quantify and compare.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Institution = pd.get_dummies(df_XGB['Institution'],drop_first=True)\nCourseSubject = pd.get_dummies(df_XGB['Course Subject'],drop_first=True)\ndf_XGB.drop(['Institution','Course Subject'],axis=1,inplace=True)\ndf_XGB = pd.concat([df_XGB,Institution,CourseSubject],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_XGB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check out if any nulls:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(df_XGB.isnull(),cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# C- XGBOOST Predictions\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1- Split data into train/test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx= df_XGB\ny=df_XGB['% Certified of > 50% Course Content Accessed']\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.4, random_state=109)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\ntrain= xgb.DMatrix(x_train,label=y_train)\ntest = xgb.DMatrix(x_test, label= y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2- XGBoost's hyperparameters\n#### At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them here. But the most common ones that you should know are:\n\n#### learning_rate: step size shrinkage used to prevent overfitting. Range is [0,1]\n#### max_depth: determines how deeply each tree is allowed to grow during any boosting round.\n#### subsample: percentage of samples used per tree. Low value can lead to underfitting.\n#### colsample_bytree: percentage of features used per tree. High value can lead to overfitting.\n#### n_estimators: number of trees you want to build.\n#### objective: determines the loss function to be used like reg:linear for regression problems, reg:logistic for classification problems with only decision, binary:logistic for classification problems with probability.\n#### XGBoost also supports regularization parameters to penalize models as they become more complex and reduce them to simple (parsimonious) models.\n\n#### gamma: controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners.\n#### alpha: L1 regularization on leaf weights. A large value leads to more regularization.\n#### lambda: L2 regularization on leaf weights and is smoother than L1 regularization.\n#### It's also worth mentioning that though you are using trees as your base learners, you can also use XGBoost's relatively less popular linear base learners and one other tree learner known as dart. All you have to do is set the booster parameter to either gbtree (default),gblinear or dart.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.5, learning_rate = 0.2,\n                max_depth = 7, alpha = 10, n_estimators = 75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_reg.fit(x_train,y_train)\npreds = xg_reg.predict(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3- Take the error ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### That's mean 5.09 per 100 error!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## ______________________________________________________________________________________","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4- Visualize Boosting Trees\n### you can also visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset. XGBoost has a plot_tree() function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the plot_tree() function along with the number of trees you want to plot using the num_trees argument.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nxgb.plot_tree(xg_reg,num_trees=0)\nplt.rcParams['figure.figsize'] = [20, 15]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5- XGBoost can count the number of times each feature is split or Feature importance features ordered according to how many times they appear. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.plot_importance(xg_reg)\nplt.rcParams['figure.figsize'] = [15,15]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sourse : [XGBoost](https://www.datacamp.com/community/tutorials/xgboost-in-python)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}