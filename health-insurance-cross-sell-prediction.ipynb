{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import datasets, ensemble\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport datetime as dt\nfrom sklearn import  metrics \nfrom sklearn.metrics import  make_scorer \nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom bayes_opt import BayesianOptimization\nfrom skopt  import BayesSearchCV \nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nimport time\nimport sys\nfrom catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train= pd.read_csv('../input/health-insurance-cross-sell-prediction/train.csv')\ntest= pd.read_csv('../input/health-insurance-cross-sell-prediction/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Response'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = [18, 30, 40, 50, 60, 70, 80, 90, 100]\nlab = ['2', '3', '4', '5', '6', '7','8','9']\ntrain['agerange'] = pd.cut(train['Age'], bins, labels = lab,include_lowest = True)\ntest['agerange'] = pd.cut(test['Age'], bins, labels = lab,include_lowest = True)\n\ntrain['agerange']=train['agerange'].apply(np.int64)\ntest['agerange']=test['agerange'].apply(np.int64)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['agerange']=train['agerange'].astype(int)\ntest['agerange']=test['agerange'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Vehicle_Damage']=train['Vehicle_Damage'].map({'Yes': 10,'No': 1}).astype(int)\ntest['Vehicle_Damage']=test['Vehicle_Damage'].map({'Yes': 10,'No': 1}).astype(int)\n\ntrain['Vehicle_Age']=train['Vehicle_Age'].map({'< 1 Year' : 0,'1-2 Year': 1,'> 2 Years': 2}).astype(int)\ntest['Vehicle_Age']=test['Vehicle_Age'].map({'< 1 Year' : 0,'1-2 Year': 1,'> 2 Years': 2}).astype(int)\n\ntrain['Vehicle_Age'].apply(np.int64)\ntest['Vehicle_Age'].apply(np.int64)\n\ntrain['Vehicle_Damage'].apply(np.int64)\ntest['Vehicle_Damage'].apply(np.int64)\n\ntrain['Vehicle_Damage_Age']= train['Vehicle_Damage']*train['Vehicle_Age']\ntest['Vehicle_Damage_Age']= test['Vehicle_Damage']*test['Vehicle_Age']\n\ntrain['mean_annual_premium_policy_sales_channel']=train.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('mean')\ntest['mean_annual_premium_policy_sales_channel']=test.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('mean')\n\ntrain['max_annual_premium_policy_sales_channel']=train.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('max')\ntest['max_annual_premium_policy_sales_channel']=test.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('max')\n\ntrain['sum_annual_premium_policy_sales_channel']=train.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('sum')\ntest['sum_annual_premium_policy_sales_channel']=test.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('sum')\n\n\ntrain['count_annual_premium_policy_sales_channel']=train.groupby(['Policy_Sales_Channel'])['id'].transform('count')\ntest['count_annual_premium_policy_sales_channel']=test.groupby(['Policy_Sales_Channel'])['id'].transform('count')\n\ntrain['min_annual_premium_policy_sales_channel']=train.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('min')\ntest['min_annual_premium_policy_sales_channel']=test.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('min')\n\ntrain['Annual_Premium_Vintage']=train['Annual_Premium']* train['Vintage']\ntest['Annual_Premium_Vintage']=test['Annual_Premium']* test['Vintage']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Gender']=train['Gender'].map({'Male' : 0,'Female': 1}).astype(int)\ntest['Gender']=test['Gender'].map({'Male' : 0,'Female': 1}).astype(int)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\nc= np.unique(train['Response'])\n\nweigh = compute_class_weight(class_weight='balanced',classes= c,  y=train['Response'])\n#weigh= pd.Series(weigh)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#46710","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['Response']==0].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train['Response']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = [x for x in train.columns if x not in ['Response', 'id']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBOOST PARAM"},{"metadata":{"trusted":true},"cell_type":"code","source":"def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=y)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=None)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], y,eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"Accuracy : %.4g\" % metrics.accuracy_score(y, dtrain_predictions))\n    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, dtrain_predprob))\n                    \n    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"predictors = [x for x in train.columns if x not in ['Response', 'id']]\nxgb1 = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=4, min_child_weight=4, \n                     gamma=0.45, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', \n                     nthread=-1, seed=27, scale_pos_weight= 7.16)\nmodelfit(xgb1, train, predictors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best Iteration: {}\".format(xgb1.get_booster().best_iteration))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_test1 = {\n    'reg_lambda': [25,30]\n}\n\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=243, max_depth=4,\n                                                  min_child_weight=4, gamma=0.45, subsample=0.8, colsample_bytree=0.8, reg_alpha= 0.01,\n                                                  reg_lambda= 25,\n                                                  objective= 'binary:logistic', nthread=4, scale_pos_weight=7.16, seed=27), \n                        param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch1.fit(train[predictors],y)\ngsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod= XGBClassifier( learning_rate =0.01, n_estimators=5000, max_depth=4,\n                   min_child_weight=4, gamma=0.45, subsample=0.8, colsample_bytree=0.8, reg_alpha= 0.01,\n                   reg_lambda= 25,objective= 'binary:logistic', nthread=4, scale_pos_weight=7.16, seed=27)\nmod.fit(train[predictors],y)\npreds= mod.predict_proba(test[predictors])[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre= pd.Series(preds)\npre.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tid=test['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.concat([tid, pre], axis=1)\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.rename({0: 'Response'},axis=1, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submissionY.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CATBOOST PARAM"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {#'depth':[3,1,2,6,4,5,7,8,9,10],\n          'iterations':[250,100,500,1000],\n          'learning_rate':[0.03,0.001,0.01,0.1,0.2,0.3], \n          #'l2_leaf_reg':[3,1,5,10,100],\n          #'border_count':[32,5,10,20,50,100,200],\n          #'ctr_border_count':[50,5,10,20,100,200],\n          #'thread_count':-1\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age','agerange',\n                        'Region_Code', 'Policy_Sales_Channel','Vehicle_Damage', \n                        'count_annual_premium_policy_sales_channel',\n                        #'count_annual_premium_region_code'\n                       ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['count_annual_premium_policy_sales_channel'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain['Region_Code']=train['Region_Code'].apply(np.int64)\ntest['Region_Code']=test['Region_Code'].apply(np.int64)\n\ntrain['Policy_Sales_Channel']=train['Policy_Sales_Channel'].apply(np.int64)\ntest['Policy_Sales_Channel']=test['Policy_Sales_Channel'].apply(np.int64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = x['Response']\nx.drop(['id','Response'],axis=1, inplace= True)\n#test.drop(['id','Response'],axis=1, inplace= True)\ntest_case_id= test['id']\ntest.drop(['id'],axis=1, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test, y_train, y_test = train_test_split(train, y,\n                                                random_state=10, test_size=0.25,\n                                                stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as cb\n#cat_features_index = [0,1,2,3,4,5,6]\n\ndef auc(m, train, test): \n    return (metrics.roc_auc_score(y_train,m.predict_proba(train)[:,1]),\n                            metrics.roc_auc_score(y_test,m.predict_proba(test)[:,1]))\n\nparams = {\n    'n_estimators' : [250],\n    'depth': [6],\n    'objective' : ['Logloss'],\n    'learning_rate' : [0.1],\n    'l2_leaf_reg': [9],\n    'bootstrap_type' : ['Bayesian'],\n    'colsample_bylevel' : [0.9],\n    'bagging_temperature' : [1],\n    'border_count':[50],\n    'num_leaves' : []\n    #'iterations': [300]\n}\ncb = cb.CatBoostClassifier(scale_pos_weight = 7.16)\ncb_model = GridSearchCV(cb, params, scoring=\"roc_auc\", cv = 3)\ncb_model.fit(train, y_train,cat_features= categorical_features)\ncb_model.cv_results_, cb_model.best_params_, cb_model.best_score_\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**VOTING CLASSIFIER**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x= train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom lightgbm  import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n#rf = RandomForestClassifier(n_estimators=50)\n\n#categorical_features = ['Gender', 'Driving_License', 'Previously_Insured','Region_Code', 'Vehicle_Age','agerange', 'Vehicle_Damage']\n\nrf_1 = LGBMClassifier(learning_rate=0.02, boosting_type='gbdt', max_depth=6,num_leaves = 32,   objective='binary', \n                      random_state=100, n_estimators=3000 ,reg_alpha=2, reg_lambda=3, n_jobs=-1, \n                      is_unbalance= True,\n                     # class_weights =weigh,\n                      categorical_feature = categorical_features)\n\nrf_2 = CatBoostClassifier(learning_rate=0.01,\n                          #subsample=0.085, \n                          depth=6, verbose=400,\n                          bootstrap_type=\"Bayesian\",border_count = 50, colsample_bylevel = 0.9, l2_leaf_reg = 9, \n                          cat_features = categorical_features,iterations=3000,\n                          #class_weights= weigh, \n                          eval_metric='AUC',loss_function= 'Logloss',\n                          bagging_temperature =  1, scale_pos_weight = 7.16, thread_count=-1\n                          #random_seed': 42\n                         )\n\nrf_3= XGBClassifier( learning_rate =0.01, n_estimators=5000, max_depth=4,\n                    min_child_weight=4, gamma=0.45, subsample=0.8, colsample_bytree=0.8, reg_alpha= 0.01,\n                    reg_lambda= 25,objective= 'binary:logistic', nthread=-1, scale_pos_weight=7.16, seed=27)\n\n\nrf = VotingClassifier(estimators=[('CatBoost_Best', rf_2),('XGBoost_Best', rf_3),], voting='soft',weights= [6,5])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold,KFold\nkf = StratifiedKFold(n_splits=5,shuffle=True,random_state=10)\nacc = []\n\nfor fold,(t_id,v_id) in enumerate(kf.split(x,y)):\n    tx = x.iloc[t_id]; ty = y.iloc[t_id]\n    vx = x.iloc[v_id]; vy = y.iloc[v_id]\n    rf.fit(tx,ty #weights = weights,\n          )\n           \n    val_y = rf.predict_proba(vx)[:,1]\n    auc_score = roc_auc_score(vy,val_y)\n    acc.append(auc_score)\n    print(f\"fold {fold} accuracy {auc_score}\")\n\nprint(f\"Mean accuracy score {np.mean(auc_score)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred= rf.predict_proba(test)[:,1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre= pd.Series(pred)\nsub = pd.concat([test_case_id, pre], axis=1)\nsub.rename({0: 'Response'},axis=1, inplace= True)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('latesub.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#With Categorical features\nclf = cb.CatBoostClassifier(eval_metric=\"AUC\", depth=10, iterations= 500, l2_leaf_reg= 9, learning_rate= 0.15)\nclf.fit(train,y_train)\nauc(clf, train, test,cat_features= categorical_features)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#With Categorical features\nclf = cb.CatBoostClassifier(eval_metric=\"AUC\",one_hot_max_size=31, \n                            depth=10, iterations= 500, l2_leaf_reg= 9, learning_rate= 0.15)\nclf.fit(train,y_train, cat_features= categorical_features)\nauc(clf, train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\nimport numpy as np\n\nmodel = CatBoostClassifier(loss_function='Logloss',scale_pos_weight=7.16)\n\ngrid = {'learning_rate': [0.001,0.01,0.1,1],\n        #'iterations': [1000,2000],\n        #'l2_leaf_reg': [1, 3, 5, 7, 9]\n       }\n\ngrid_search_result = GridSearchCV(estimator=model, param_grid = grid, scoring= 'roc_auc')\n\ngrid_search_result.fit(train, y)\n\ngrid_search_result.cv_results_, grid_search_result.best_params_, grid_search_result.best_score_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"clf = CatBoostClassifier()\n\ncat_dims = [train.columns.get_loc(i) for i in categorical_features[:-1]] \nclf.fit(train, np.ravel(y), cat_features=categorical_features)\nres = clf.predict(test)\nprint('error:',1-np.mean(res==np.ravel(test_label)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def catboost_param_tune(params,train_set,train_label,cat_dims=None,n_splits=3):\n    ps = paramsearch(params)\n    # search 'border_count', 'l2_leaf_reg' etc. individually \n    #   but 'iterations','learning_rate' together\n    for prms in chain(ps.grid_search(['border_count']),\n                      ps.grid_search(['ctr_border_count']),\n                      ps.grid_search(['l2_leaf_reg']),\n                      ps.grid_search(['iterations','learning_rate']),\n                      ps.grid_search(['depth'])):\n        res = crossvaltest(prms,train_set,train_label,cat_dims,n_splits)\n        # save the crossvalidation result so that future iterations can reuse the best parameters\n        ps.register_result(res,prms)\n        print(res,prms,'best:',ps.bestscore(),ps.bestparam())\n    return ps.bestparam()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crossvaltest(params,train_set,train_label,cat_dims,n_splits=5):\n    kf = KFold(n_splits=n_splits,shuffle=True) \n    res = []\n    for train_index, test_index in kf.split(train_set):\n        train = train_set.iloc[train_index,:]\n        test = train_set.iloc[test_index,:]\n\n        labels = train_label.ix[train_index]\n        test_labels = train_label.ix[test_index]\n\n        clf = cb.CatBoostClassifier(**params)\n        clf.fit(train, np.ravel(labels), cat_features=cat_dims)\n\n        res.append(np.mean(clf.predict(test)==np.ravel(test_labels)))\n    return np.mean(res)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}