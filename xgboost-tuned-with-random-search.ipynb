{"cells":[{"metadata":{"trusted":false,"_uuid":"657791acd2ee134894b9f42ce4a9c5fd999f643a"},"cell_type":"markdown","source":"# Purpose of this Kernel\n\nThe following is a demonstration machine learning project in Python. I have a couple years of experience with R and have thus become familiar with machine learning using the Caret framework. However, I realize that Python is truely the lingua franca of data science so I am getting on board. I have a good deal of Python porgramming experience so (hopefully) the transition will be fairly painless. My goal for this kernel isn't to demonstrate anything ground breaking but to get personally comfortable creating attractive looking kernels in python and develope a XGBoost workflow that I can come back to in future more complex projects.  \n\n**All feedback is highly appreciated!**\n\nI have drawn on a large number of resources to learn Python and XGBoost but the following had a direct effect on this kernel:\n\n* This [cheat sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf) for Scikit Learn \n* This [guide](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) to tuning XGBoost hyperparamters \n* This [kernel](https://www.kaggle.com/tilii7/hyperparameter-grid-search-with-xgboost) by Tilii was really helpful in understanding how to implement random grid search\n* This [mini course](https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/) on XGBoost in Python \n\n"},{"metadata":{"_uuid":"657e209fb76e0d59683f025bb354849ac1e4c54a"},"cell_type":"markdown","source":"<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Python.svg/768px-Python.svg.png\" width=\"250px\"/>"},{"metadata":{"_uuid":"0238bf018d5352b58cbeaf6bb9710e00c1a8935b"},"cell_type":"markdown","source":" <a id=\"top\"></a>\n# Table of Contents\n\n* [Load Packages](#1)\n* [Import Data](#2)\n* [Preprocess Data](#3)\n* [Split Data Into Test and Training Sets](#4)\n* [Build Base Model](#5)\n* [Build Tuned Model](#6)\n* [Conclusions](#7)\n"},{"metadata":{"_uuid":"33762eaf6ab964188fe856272c8398ceb461ca99"},"cell_type":"markdown","source":"# Load Packages <a id=\"1\"></a>"},{"metadata":{"trusted":false,"_uuid":"6ede47ee909c3baec28e89494e346b2ae4054be6"},"cell_type":"code","source":"# pandas is for data manipulation and wrangling\nimport pandas as pd\n# XGBoost is the specific model and we want the classifier \nfrom xgboost import XGBClassifier\n# creates feature importance plot\nfrom xgboost import plot_importance\n# Label encoding transforms non-ordinal catigorical variables\nfrom sklearn.preprocessing import LabelEncoder\n# splits data into test and training sets\nfrom sklearn.model_selection import train_test_split\n# for tuning, located in sklearn.grid_search depending on version\nfrom sklearn.model_selection import RandomizedSearchCV\n# for assessing accuracy\nfrom sklearn.metrics import accuracy_score\n# For the spliting the data\nfrom sklearn.model_selection import StratifiedKFold\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e5f1bbd72eca2be05b2485b94003fe53abb0f28"},"cell_type":"markdown","source":"# Import Data <a id=\"2\"></a>"},{"metadata":{"trusted":true,"_uuid":"703ed4e11a4991d3d7841f5ee3e6a30333e7b1ff"},"cell_type":"code","source":"# import data set\ndf = pd.read_csv('../input/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n# view the top rows\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f1d257fe0fe5462756b2759905fe37d94eae85e"},"cell_type":"markdown","source":"# Preprocess Data <a id=\"3\"></a>"},{"metadata":{"_uuid":"162043654fededc733eaeb9cc1cd0f3cfc655aa6"},"cell_type":"markdown","source":"Fortunately this is not a 'messy' data set in that there aren't any missing values. That said, in order to use XGBoost some preprocessing still needs to be done so that all the data is numerical:\n\n* Encode catigorical variables with two levels.\n* For catigorical variables with more than two levels create dummy variables.\n* Remove the customer ID feature.\n* The data in the Total Charges column are strings. Convert to float"},{"metadata":{"trusted":true,"_uuid":"100196fa62e950f3e6888da18d457e07efa94fea"},"cell_type":"code","source":"# Make dummy variables for catigorical variables with >2 levels\ndummy_columns = [\"MultipleLines\",\"InternetService\",\"OnlineSecurity\",\n                 \"OnlineBackup\",\"DeviceProtection\",\"TechSupport\",\n                 \"StreamingTV\",\"StreamingMovies\",\"Contract\",\n                 \"PaymentMethod\"]\n\ndf_clean = pd.get_dummies(df, columns = dummy_columns)\n\n# Encode catigorical variables with 2 levels\nenc = LabelEncoder()\nencode_columns = [\"Churn\",\"PaperlessBilling\",\"PhoneService\",\n                  \"gender\",\"Partner\",\"Dependents\"]\n\nfor col in encode_columns:\n    df_clean[col] = enc.fit_transform(df[col])\n    \n# Remove customer ID column\ndel df_clean[\"customerID\"]\n\n\n# Make TotalCharges column numeric, empty strings are zeros\ndf_clean[\"TotalCharges\"] = pd.to_numeric(df_clean[\"TotalCharges\"],\n    errors = 'coerce').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49f6b1b3a645bfc1351a2492a47e2f68d187060b"},"cell_type":"markdown","source":"# Split Data into Test and Training Sets <a id=\"4\"></a>"},{"metadata":{"_uuid":"5f6acc1e03c58416d0a938d9856869c2330a1d7c"},"cell_type":"markdown","source":"Split the data into to the target variable, y, which needs to be predicted (whether the customer churned or not) and all the other predictive variables x. Then use the train_test_split function to assign 80% of the data to the training set and 20% to the test set."},{"metadata":{"trusted":true,"_uuid":"95c222ce5cec0ed43297dcb3104b9a0122c18559"},"cell_type":"code","source":"# Split data into x and y\ny = df_clean[[\"Churn\"]]\nx = df_clean.drop(\"Churn\", axis=1)\n\n# Create test and training sets\nx_train, x_test, y_train, y_test = train_test_split(x,\n    y, test_size= .2, random_state= 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"738327c16b4a4d61d5680472403414e32bbd3c4e"},"cell_type":"markdown","source":"# Build Base Model <a id=\"5\"></a>"},{"metadata":{"_uuid":"57844cf8a353f77cc16fee108d2253a0add54b8f"},"cell_type":"markdown","source":"This is the XGBoost model built using the default parameters. I will compare this base model's performance to that of a tuned model."},{"metadata":{"trusted":true,"_uuid":"1ca1af00467252bf58c2996a2713e4e55c6780f7"},"cell_type":"code","source":"# Build XGBoost model\nmodel = XGBClassifier()\nmodel.fit(x_train, y_train)\n\n\n# make predictions for test data\ny_pred = model.predict(x_test)\npredictions = [round(value) for value in y_pred]\n\n# Find Accuracy\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\n# Display feature importance\nplot_importance(model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f58435032f4c7873e9635ced4540d74ba3b69fe"},"cell_type":"markdown","source":"# Build Tuned Model <a id=\"6\"></a>\n"},{"metadata":{"_uuid":"0052e46f3b32908514855dcc3b4ff3e0597d5dd8"},"cell_type":"markdown","source":"The following explinations are quick reminders to myself based on the documentation. \n\n### Model parameters to be tuned\n* **min_child_weight** – Minimum sum of instance weight (hessian) needed in a child. Used to control over-fitting. Values that are too high can lead to under-fitting. If the classes are highly unbalanced, lower values (even 1) can be alright. \n* **gamma** – Minimum loss reduction required to make a further partition on a leaf node of the tree. Good values of this parameter are highly specific to the data and model. (typically 0 - 10) \n* **subsample** – Subsample ratio of the training instance. Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting. (typically .5 - 1). \n* **colsample_bytree** – Subsample ratio of columns for each split, in each level. Denotes the fraction of columns to be randomly sampled for each tree. (typically .5 - 1)\n* **max_depth** – Maximum tree depth for base learners. Used to control overfitting. (typically 3 – 10)\n\n### Model parameters that do not need to be tuned\n* **learning_rate** – Boosting learning rate (xgb’s “eta”), smaller generally gives better results but will take more time\n* **n_estimators** – Number of boosted trees to fit. To a point, more are better but will take more time.\n* **objective** – Specify the learning task and the corresponding learning objective. I use ‘binary:logistic’ which uses logistic regression and returns probabilities or each of the two classes. \n* **silent** – Whether to print messages while running boosting.\n* **nthread** – Number of parallel threads used to run xgboost. -1 means all cores available.\n* **booster** – Specify which booster to use: gbtree, gblinear or dart. As this is a classification model we cannot use gblinear. I choose gbtree arbitrarily but I know others have had success with dart. \n\n### Stratified K fold parameters \nWe use stratified folds so that each sub-sample has the same ratio of each target variable as the entire data set. We do this because machine learning model performance can be very sensitive to sample balancing. \n* **n_splits** – Number of folds, higher will generally return better results but will take more time to run.\n* **shuffle** – Whether to shuffle each stratification of the data before splitting into batches. These is no need to do so here as the data ordering is arbitrary. \n* **random_state** - If int, random_state is the seed used by the random number generator.\n\n### Random search parameters\n* **estimator** - A object of that type is instantiated for each grid point. This is the model we specify,\n* **param_distributions** – Dictionary with parameter names as keys and distributions or lists of parameters to try. This is specified as ‘tuned parameters’ in this kernel.\n* **n_iter** – Number of parameter settings that are sampled. In other words, the number of random combinations of the tuning parameters to evaluate the model on. The higher this number, the better the results but the longer the run time. \n* **scoring** – A single string to evaluate the predictions of the test set\n* **n_jobs** – number of jobs to run in parallel. -1 means all cores available.\n* **cv** - Determines the cross-validation splitting strategy. Here I use a split method which generates indices for a training and testing set. \n* **verbose** - Controls the verbosity: the higher, the more messages.\n* **random_state**  - random_state is the seed used by the random number generator\n"},{"metadata":{"trusted":true,"_uuid":"f36b6f37ecce70d22fda2c64a861ba2ab103e3cd"},"cell_type":"code","source":"tuned_parameters = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5, 10],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 5, 8]\n        }\n\nmodel = XGBClassifier(learning_rate=0.02, \n                    n_estimators=200,\n                    booster = 'gbtree',\n                    objective='binary:logistic',\n                    silent=True, \n                    nthread=-1)\n\n\nskf = StratifiedKFold(n_splits=5, shuffle = False, random_state = 22)\n\nrandom_search_model = RandomizedSearchCV(estimator = model, \n                                   param_distributions=tuned_parameters, \n                                   n_iter=10, \n                                   scoring='accuracy', \n                                   n_jobs=-1, \n                                   cv=skf.split(x_train,y_train), \n                                   verbose=3, \n                                   random_state=22)\n\nrandom_search_model.fit(x_train, y_train)\n\ny_pred = random_search_model.predict(x_test)\n\npredictions = [round(value) for value in y_pred]\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b33eb97022b157820307f11018605572d43c10d2"},"cell_type":"markdown","source":"# Conclusions <a id=\"7\"></a>"},{"metadata":{"_uuid":"977552d49b0fca92708cacdd61cedaabff977b4d"},"cell_type":"markdown","source":"This kernel has successfully executed a simple machine learning problem and can be easily adapted in the future. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}