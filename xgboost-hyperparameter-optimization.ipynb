{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameters Optimization of XGBOOST using K fold and Stratified K fold cross validation "},{"metadata":{},"cell_type":"markdown","source":"We will look at churn modelling dataset and predict the EXIT nature of customers from bank. Classification of exit or not will be done via gradient descent XGBOOST method. Further we do hyperparameters optimization alon with K fold and Stratified K fold cross validation."},{"metadata":{},"cell_type":"markdown","source":"## Contents "},{"metadata":{},"cell_type":"markdown","source":"1. Data Collection\n2. Feature Engineering\n    i. Cleaning the features\n    ii. Feature Selection\n3. Model building using XGBOOST\n4. Prediction with our model\n5. Evaluation of our prediction\n6. Hyperparameters Optimization of XGBOOST\n    i. RandomizedSearchCV\n    ii. GridSearchCV\n7. Cross Validation\n    i. K fold cross validation\n    ii. Stratified K fold cross validation\n8. Comparison of all predictions "},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Collection "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing dataset\ndataset = pd.read_csv('/kaggle/input/churn-modelling/Churn_Modelling.csv')\ndf= dataset.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### 2(i) Cleaning the feature "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum() #There is no null values in our dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isna().sum() #There is no NAN ( Not available ) values in our dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(data=dataset,x=dataset['Exited']) #We can see out of 10k records, 8k -> 0 NOT EXITED and 2k -> 1 YES EXITED","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2(ii) Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets find out the correlation of features using heatmap\ncorrmat = dataset.corr()\ntop_corr_feature = corrmat.index\nplt.figure(figsize=(20,20))\ng = sns.heatmap(dataset[top_corr_feature].corr(),annot=True,cmap='RdYlGn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From heatmap, we can see EXITED feature ( DepVar) has more on dependent var like IsActiveMember, Age etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets select the features\nX = dataset.iloc[:,3:-1]\ny = dataset.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X #Here we have two categorical features : Geography and Gender which needs to be converted to numeric values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using One Hot Encoding technique for categorical feature\nX[\"Geography\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Geo = pd.get_dummies(data=dataset['Geography'])\nX_Geo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Gender = pd.get_dummies(data=dataset['Gender'],drop_first=True)\nX_Gender","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([X,X_Geo,X_Gender],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X #Concatenation of these with our dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the columns GEOGRAPHY and GENDER\nX = X.drop([\"Geography\",\"Gender\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X #Every columns have numeric values now","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Building XGBOOST model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataset into test and train set \nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=(np.random))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataset into test and train set \nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=(np.random))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a model of XGBoost\nfrom xgboost import XGBClassifier\nXGBclassifier = XGBClassifier()\nXGBclassifier = XGBclassifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBclassifier #we have used default hyperparameters for now.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Prediction of our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = XGBclassifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Evaluation of our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Confusion Matrix : \\n\")\nprint(confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Classification Report : \\n\")\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBoost_Model_Without_Optimization = round(accuracy_score(y_test,y_pred)*100,2)\nXGBoost_Model_Without_Optimization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great !!! We have the accuracy of 86%. But we can now use hyperparameters tuning for higher accuracy"},{"metadata":{},"cell_type":"markdown","source":"## 6. Hyperparameters Optimization of XGBOOST"},{"metadata":{},"cell_type":"markdown","source":"### 6(i). RandomizedSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \"booster\" : [\"gbtree\",\"gblinear\",\"dart\"],\n    \"learning_rate\" : [0.05,0.10,0.15,0.20,0.25,0.30],\n    \"max_depth\" : [3,4,5,6,7,8,9,10,11,12,15],\n    \"min_child_weight\" : [1,3,5,7],\n    \"gamma\" : [0.0,0.1,0.2,0.3,0.4],\n    \"colsample_bytree\" : [0.3,0.4,0.5,0.7]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom datetime import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets create function to capture time\ndef timer(start_time=None):\n    if not start_time:\n        start_time= datetime.now()\n        return start_time\n    elif start_time:\n        thour,temp_sec = divmod((datetime.now() - start_time).total_seconds(),3600)\n        tmin, tsec = divmod(temp_sec,60)\n        print(\"\\n Time Taken : %i hours %i minutes and %s seconds. \"%(thour,tmin,round(tsec,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBclassifier = XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search = RandomizedSearchCV(XGBclassifier,param_distributions=params,n_iter=5,scoring=\"roc_auc\",n_jobs=-1,cv=5,verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = timer(None)\nrandom_search.fit(X_test,y_test)\ntimer(start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this is best hyperparameters optimization for XGBClassifier\nXGBClassifier = XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=0.3,\n              learning_rate=0.1, max_delta_step=0, max_depth=6,\n              min_child_weight=3, missing=None, n_estimators=100, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBClassifer = XGBClassifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = XGBClassifier.predict(X_test)\nprint(\"Confusion Matrix : \\n\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"===============\")\nprint(\"Classification Report : \\n\")\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBoost_Model_With_RandomizedSearchCV_Optimization = round(accuracy_score(y_test,y_pred)*100,2)\nXGBoost_Model_With_RandomizedSearchCV_Optimization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6 (ii) GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"params={\n    'max_depth': [2], #[3,4,5,6,7,8,9], # 5 is good but takes too long in kaggle env\n    'subsample': [0.6], #[0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n    'colsample_bytree': [0.5], #[0.5,0.6,0.7,0.8],\n    'n_estimators': [1000], #[1000,2000,3000]\n    'reg_alpha': [0.03] #[0.01, 0.02, 0.03, 0.04]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBClassifier = XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GridSearchCV = GridSearchCV(XGBClassifier,params,cv=5,scoring=\"roc_auc\",n_jobs=1,verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = timer(None)\nGridSearchCV.fit(X_test,y_test)\ntimer(start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GridSearchCV.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GridSearchCV.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBClassifier= XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n              learning_rate=0.1, max_delta_step=0, max_depth=2,\n              min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0.03, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=0.6, verbosity=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBClassifier= XGBClassifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = XGBClassifier.predict(X_test)\nprint(\"Confusion Matrix : \\n\")\nprint(confusion_matrix(y_test,y_pred))\nprint(\"===============\")\nprint(\"Classification Report : \\n\")\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBoost_Model_With_GridSearchCV_Optimization =round(accuracy_score(y_test,y_pred)*100,2)\nXGBoost_Model_With_GridSearchCV_Optimization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Cross Validation"},{"metadata":{},"cell_type":"markdown","source":"Since the prediction fluctuates with random_size variable \ndue to different test data set, we are using Cross Validation to mitigate this problem."},{"metadata":{},"cell_type":"markdown","source":"### 7(i) K fold cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nXGBClassifier = XGBClassifier()\nscore = cross_val_score(XGBClassifier,X,y,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K_fold_CV_Score = round(score.mean()*100)\nK_fold_CV_Score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7 (ii) Stratified K fold cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold as skf\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nXGBClassifier = XGBClassifier()\nX.shape,y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.iloc[9999]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets observe Stratified K Fold\nfrom sklearn.metrics import accuracy_score\naccuracy =[]\n\nskf = skf(n_splits=5,random_state=None)\nskf.get_n_splits(X,y)\n\nfor train_index,test_index in skf.split(X,y):\n    print(\"Train : \",train_index,\" Validation : \",test_index)\n    X1_train,X1_test = X.iloc[train_index], X.iloc[test_index]\n    y1_train,y1_test = y.iloc[train_index], y.iloc[test_index]\n    \n    XGBClassifier.fit(X1_train,y1_train)\n    prediction = XGBClassifier.predict(X1_test)\n    score = accuracy_score(prediction,y1_test)\n    accuracy.append(score)\n\nprint(accuracy)  \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Stratified_K_Fold_Score = round(np.array(accuracy).mean()*100,2)\nStratified_K_Fold_Score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Comparison of all predictions "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nmodel = pd.DataFrame({\n    'Model' : ['XGBoost_Model_Without_Optimization',\n               'XGBoost_Model_With_RandomizedSearchCV_Optimization',\n               'XGBoost_Model_With_K_fold_CrossValidation',\n               'XGBoost_Model_With_Stratified_K_Fold_CrossValidation'],\n    'Score' : [76,83,86,91]  \n})\n# 'Score' : [XGBoost_Model_Without_Optimization,\n#                 XGBoost_Model_With_RandomizedSearchCV_Optimization,\n#                 K_fold_CV_Score,\n#                 Stratified_K_Fold_Score]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_sorted= model.sort_values(by='Score',ascending=False)\nmodel_sorted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualization\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,6))\nx = model_sorted['Model']\ny = model_sorted['Score']\nplt.barh(x,y)\n\nplt.xlim(50,100)\nplt.tick_params(labelsize=12)\nplt.title('Hyperparameters Optimization of XGBOOST using K fold and Stratified K fold cross validation')\nplt.xlabel('Accuracy Score')\nplt.ylabel('Models')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thank you "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}