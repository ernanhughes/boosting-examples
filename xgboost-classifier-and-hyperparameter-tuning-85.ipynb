{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classification with XGBoost and hyperparameter optimization"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import required libraries for data analysis\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# open csv file\ndf = pd.read_csv('/kaggle/input/indian-liver-patient-records/indian_liver_patient.csv')\ndf.columns = df.columns.map(str.lower)                              # column names to lowercase\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check data types, it provides also details on null values, so next checking of null values may not be required\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all').T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What can we see from describe & info ?\n- for gender we have 2 unique values - sounds reasonable\n- there seems to be extremes for several fields (compare 75% with max) - we will have to fix it\n- dataset, our target variable, has 2 values\n- albumin_and_globulin_ratio have some missing values those must be handled"},{"metadata":{},"cell_type":"markdown","source":"### Handle missing values\nWe are going to fix missing values on albumin_and_globulin_ratio field, easiest way is often best way, so just replace it with mean or median"},{"metadata":{"trusted":true},"cell_type":"code","source":"# replacing missing values with mean\ndf.albumin_and_globulin_ratio.fillna(df.albumin_and_globulin_ratio.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# are there still any missing values?\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's start exploratory data analysis (EDA)\nMost datascientist will tell you that EDA is most funny part of datascience work. You are exploring different relations between data and how they interact... so do it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation between variables\nsns.set()\nsns.pairplot(df, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation between variables\nsns.set()\nsns.pairplot(df, hue='dataset', kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation with dataset - target value\ndf.corr()['dataset']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# full correlation table\ndf.corr().style.background_gradient(cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Outcome?** I've seen in many datasets there is none. Data scientist just simply show correlation and skip any actions to be done... not in this case! \nWe have strong correlation between some variables:\n- direct_bilirubin & total_bilirubin\n- aspartate_aminotransferase & alamine_aminotransferase\n- total_protiens & albumin\n- albumin_and_globulin_ratio & albumin\n\nWe will drop some of them as features should be independent"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is simply my selection (from highly correlated features, you could also use different from pairs)\ndf.drop(['direct_bilirubin', 'aspartate_aminotransferase', 'total_protiens', 'albumin'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outliers & transformation\nNow check data using boxplot and distplot to see what features are skewed, what is ratio of outliers, if log1p helps etc"},{"metadata":{"trusted":true},"cell_type":"code","source":"# outlier check\nplt.figure(figsize=(15, 20))\n\nfor i, c in enumerate(df.drop('dataset', axis=1).select_dtypes(include='number').columns):\n    plt.subplot(10,2,i*2+1)\n    sns.boxplot(df[c], color='blue')\n    plt.title('Distribution plot for field:' + c)\n    plt.xlabel('')\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n\n    \n    plt.subplot(10,2,i*2+2)\n    sns.boxplot(df[c].apply('log1p'), color='red')\n    plt.title('Log1p distribution plot for field:' + c)\n    plt.xlabel('')\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 12))\n\nfor i, c in enumerate(df.select_dtypes(include='number').columns):\n    plt.subplot(5,2,i+1)\n    sns.distplot(df[c])\n    plt.title('Distribution plot for field:' + c)\n    plt.xlabel('')\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### What can we get out of dist plot and box plot?\n- dataset is slightly imbalanced\n- slightly skewed features: albumin_and_globulin_ration\n- strongly skewed features: total_bilirubin, direct_bilirubin, alkaline_phosphotase, alamine_aminotransferase, aspartate_aminotransferase\n\nWe will fix these values using log1p transformation and then scale variables using RobustScaler as this one is good for data with outliers. If you think this is not good procedure, let me know!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# save skewed features\nskewed_cols = ['albumin_and_globulin_ratio','total_bilirubin', 'alkaline_phosphotase', 'alamine_aminotransferase']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply log1p transformation on dataframe - just selected values\nfor c in skewed_cols:\n    df[c] = df[c].apply('log1p')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next check & fix strongly skewed features\n# apply log1p transform\nplt.figure(figsize=(15, 12))\n\nfor i, c in enumerate(skewed_cols):\n    plt.subplot(5,2,i+1)\n    sns.distplot(df[c].apply(np.log1p))\n    plt.title('Distribution plot for field:' + c)\n    plt.xlabel('')\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode & Scale\nI will use sklearn library to encode gender and scale numerical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, RobustScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gender contains string values Male, Female; these will be converted into 0, 1, as ML algorithms like just numerical values\nle = LabelEncoder()\ndf['gender'] = le.fit_transform(df['gender'])\ndf.gender.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs = RobustScaler()\nfor c in df[['age', 'gender', 'total_bilirubin', 'alkaline_phosphotase', 'alamine_aminotransferase', 'albumin_and_globulin_ratio']].columns:\n    df[c] = rs.fit_transform(df[c].values.reshape(-1, 1))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Balance data\nAs our dataset is imbalanced, will use sklearn's sample to have same ratio of target variables (we could use also SMOTE from imblearn).\nThis may not be needed each time as some algorithms already have techniques for imbalanced datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\ndf.dataset.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data on majority and minority.. minority is dataset == 2\nminority = df[df.dataset==2]\nmajority = df[df.dataset==1]\n\nprint('Minority size:', minority.shape)\nprint('Majority size:', majority.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# choosing upsample as even now we do not have too much data\nminority_upsample = resample(minority, replace=True, n_samples=majority.shape[0])\nprint('Minority upsampled size:', minority_upsample.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge majority with upsampled minority\ndf = pd.concat([minority_upsample, majority], axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split data for modeling\nThis is very needed in order to be able to compare performance of model on unseen data. I will choose test size to be 0.25. Dataset is also split to X (features) and y (target) variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.drop('dataset', axis=1), df['dataset'], test_size=0.25, random_state=123)\n\nprint('Train values shape:', X_train.shape)\nprint('Test values shape:', X_test.shape)\nprint('Train target shape:', y_train.shape)\nprint('Test target shape:', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model selection\nWe will use several popular models and see how they perform on our dataset.\nAt the end we choose 3 best performing models and will merge them together."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, Logistic Regression worked fine, nothing special, but still 0.62 f1 score on test dataset is pretty well, model also does not overfit. But we want better score, let's try now SVM that should work well on low dimension data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machines\nmodel = SVC()\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK, f1 increased to 0.64, model is not overtrained, even roc_auc is higher so our model is able to do classification better, SVM works fine.... but it's pretty much same as logistic regression, isn't it!? Let's try famous random forest classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest\nmodel = RandomForestClassifier(n_jobs=-1,random_state=123)\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hell yeah! That's why it's so famous, got 0.84 f1 on test dataset, roc_auc reaching 0.85, all looks great just on fact model is overfitting training data... however, still generalize fine on test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Neural nets\nmodel = MLPClassifier()\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not so good as random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-Neighbors\nmodel = KNeighborsClassifier()\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also not so good"},{"metadata":{},"cell_type":"markdown","source":"... which one is most favorite machine learning model on Kaggle? XGBoost! ... let's try it, unfortunately it's not part of sklearn so must be installed separately (or is, but in slightly modified version)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier(random_state=123)\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice one, XGBoost is catching random forest in performance!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extra Trees\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier(random_state=123)\nmodel.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Incredible! Extra Trees has definitely best performance from all models we've tried:\n- 86% precission\n- 86% recall\n- 0.86 f1\n- 0.85 roc_auc"},{"metadata":{},"cell_type":"markdown","source":"## Model evaluation & optimization\nAs our dataset is not too big, we will use GridSearchCV for parameter tuning, in case of large datasets RandomizedSearchCV could be better. Our main objective will be to improve roc_auc score and avoid overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random forest\n# n_jobs=-1 to allow run it on all cores\nparams = {\n    'n_estimators': [100, 200, 500],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [1,2,4,5],\n    'min_samples_leaf': [1,2,4,5],\n    'max_leaf_nodes': [4,10,20,50,None]\n}\n\ngs1 = GridSearchCV(RandomForestClassifier(n_jobs=-1), params, n_jobs=-1, cv=KFold(n_splits=3), scoring='roc_auc')\ngs1.fit(X_train, y_train)\n\nprint('Best score:', gs1.best_score_)\nprint('Best score:', gs1.best_params_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGBoost\n# n_jobs=-1 to allow run it on all cores\nparams = {\n    'n_estimators': [100, 200, 500],\n    'learning_rate': [0.01,0.05,0.1],\n    'booster': ['gbtree', 'gblinear'],\n    'gamma': [0, 0.5, 1],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [0.5, 1, 5],\n    'base_score': [0.2, 0.5, 1]\n}\n\ngs2 = GridSearchCV(XGBClassifier(n_jobs=-1), params, n_jobs=-1, cv=KFold(n_splits=3), scoring='roc_auc')\ngs2.fit(X_train, y_train)\n\nprint('Best score:', gs2.best_score_)\nprint('Best score:', gs2.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extra Tree\n# n_jobs=-1 to allow run it on all cores\nparams = {\n    'n_estimators': [100, 200, 500],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [1,2,4,5],\n    'min_samples_leaf': [1,2,4,5],\n    'max_leaf_nodes': [4,10,20,50,None]\n}\n\ngs3 = GridSearchCV(ExtraTreesClassifier(n_jobs=-1), params, n_jobs=-1, cv=KFold(n_splits=3), scoring='roc_auc')\ngs3.fit(X_train, y_train)\n\nprint('Best score:', gs3.best_score_)\nprint('Best score:', gs3.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Voting - we will use these 3 models in final, merge them\nObjective here is not to make more accurate model (but it can be), however we want to more stable model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nfrom sklearn.ensemble import VotingClassifier\n\n# votes = [\n#     ('rf', RandomForestClassifier(n_jobs=-1, criterion='gini', max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100)),\n#     ('xgb', XGBClassifier(n_jobs=-1, base_score=0.2, booster='gbtree', gamma=0, learning_rate=0.1, n_estimators=500, reg_alpha=0, reg_lambda=1)),\n#     ('xt', ExtraTreesClassifier(n_jobs=-1, criterion='gini', max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500))\n# ]\n\nvotes = [\n    ('rf', gs1.best_estimator_),\n    ('xgb', gs2.best_estimator_),\n    ('xt', gs3.best_estimator_)\n]\n\n# soft voting based on weights\nvotesClass = VotingClassifier(estimators=votes, voting='soft', n_jobs=-1)\nvotesClass_cv = cross_validate(votesClass, X_train, y_train, cv=KFold(3, random_state=123))\nvotesClass.fit(X_train, y_train)\n\nvotesClass_cv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = votesClass\n#model.fit(X_train, y_train)\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint(model)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Great**, we've got f1 score 0.84, it's not better that we've had, but even not worse.\nOur model is still overfitting data, it could be much better having more data but model still generalize well on test set.\nIf you have any suggestions how to reduce overfitting, place your comment here ;)"},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nIn this kernel I've tried to present basic work of data scientist. As I am just starting, excuse possible mistakes and propose better solutions in comment section.\n\nThis is my **very first kernel on kaggle**, give me thumb up if you enjoyed at least a bit ;)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}