{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom matplotlib import colors \nfrom matplotlib.ticker import PercentFormatter \nimport numpy as np \nimport matplotlib.pyplot as plt \n\nimport os\nprint(os.listdir(\"../input\"))\n\nabnormal = pd.read_csv(\"../input/ptbdb_abnormal.csv\", header = None) \nnormal = pd.read_csv(\"../input/ptbdb_normal.csv\", header = None)\n\nabnormal = abnormal.drop([187], axis=1)\nnormal = normal.drop([187], axis=1)\n\nabnormal.head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flatten_ab_y = (abnormal.values)\nflatten_ab_y  = flatten_ab_y[:,5:70].flatten()\n\nprint(flatten_ab_y.shape)\n\nab_x=np.arange(0,65)\nab_x = np.tile(ab_x, abnormal.shape[0])\n\nplt.hist2d(ab_x, flatten_ab_y, bins = (65,100), cmap = plt.cm.jet) \n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above histogram color map for PTB data marked as abnormal, you can infer that most of the ECG features is widely distributed in the range of 0 - 0.4 and there is no fixed pattern to this data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot((abnormal.values)[0][5:70])\nplt.show()\n\nplt.plot((abnormal.values)[50][5:70])\nplt.show()\n\nplt.plot((abnormal.values)[117][5:70])\nplt.show()\n\nplt.plot((abnormal.values)[1111][5:70])\nplt.show()\n\nplt.plot((abnormal.values)[100][5:70])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above, you can see that PTB data marked as abnormal donot have any fixed pattern in the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"flatten_norm_y = normal.values\nflatten_norm_y  = flatten_norm_y[:,5:70].flatten()\n\nnorm_x=np.arange(0,65)\nnorm_x = np.tile(norm_x, normal.shape[0])\n\nplt.hist2d(norm_x,flatten_norm_y, bins=(65,100), cmap=plt.cm.jet)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above histogram color map for PTB data marked as normal, you can infer that the graph of ECG features follow a standard bell shape and they peak in between the features 20-30**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot((normal.values)[0][5:70])\nplt.show()\n\nplt.plot((normal.values)[50][5:70])\nplt.show()\n\nplt.plot((normal.values)[117][5:70])\nplt.show()\n\nplt.plot((normal.values)[1111][5:70])\nplt.show()\n\nplt.plot((normal.values)[100][5:70])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above, you can see that PTB data marked as normal has a fixed bell shape in the data and it peaks between the features 20-30 **"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_abnormal = np.ones((abnormal.shape[0]))\ny_abnormal = pd.DataFrame(y_abnormal)\n\ny_normal = np.zeros((normal.shape[0]))\ny_normal = pd.DataFrame(y_normal)\n\nX = pd.concat([abnormal, normal], sort=True)\ny = pd.concat([y_abnormal, y_normal] ,sort=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(abnormal.dtypes, normal.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abnormal.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normal.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check any of the features have a null**"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.any(X_train.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.any(X_test.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Evaluate a bunch of ML Models against the test data, we find that XGBoost performs the best***"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed=123\n\nclassifiers = [\n    LogisticRegression(class_weight='balanced', random_state=seed),\n    KNeighborsClassifier(3, n_jobs=-1),\n    SVC(gamma='auto', class_weight='balanced', random_state=seed),\n    RandomForestClassifier(random_state=seed, n_estimators = 1000),\n    MLPClassifier(alpha=1, max_iter=1000),\n    XGBClassifier(learning_rate =0.1,n_estimators=1000, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8,\n                  colsample_bytree=0.8, objective= 'binary:logistic',nthread=4, scale_pos_weight=1,seed=seed)\n]\n\nnames = [\"Logistic\", \"Nearest Neighbors\", \"RBF SVM\", \"Random Forest\", \"Neural Net\", \"XGB\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\nfrom sklearn.utils.validation import column_or_1d\n\nfor name, clf in zip(names, classifiers):\n        \n    y_train = column_or_1d(y_train, warn=True)\n    clf.fit(X_train, y_train)\n    print(f\"{name}: {round(accuracy_score(y_test, clf.predict(X_test)),3)}\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"clf = XGBClassifier(learning_rate =0.1,n_estimators=1000, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8,\n                  colsample_bytree=0.8, objective= 'binary:logistic',nthread=4, scale_pos_weight=1,seed=seed)\n\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.reshape(y_pred, (y_pred.shape[0],1))\n\ny_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test)\n\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"Accuracy:\", accuracy)\nprint(\"f1:\", f1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Display a feature importance graph. The top 18 features by importance are as follows:\n\n*4  32 137  50  43 112  29 124 108 113 165  33  84 151  46 149  69  31**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature importance\nfrom matplotlib import pyplot as plt\n\nfeature_imp = np.argsort(clf.feature_importances_)\nprint(np.flip(feature_imp))\n\n# plot\nplt.figure(figsize=(20,8))\n\nplt.bar(range(len(clf.feature_importances_)), clf.feature_importances_)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}