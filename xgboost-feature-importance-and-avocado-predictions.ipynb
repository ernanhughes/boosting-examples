{"cells":[{"metadata":{"_uuid":"6beb1d2918296b1f2ffce0fb66b07bd0910af12d"},"cell_type":"markdown","source":"Feature importance, hyperparameter tuning and further usage of a ml model\n======================\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom numpy import sort\nimport seaborn as sns\nfrom sklearn.externals import joblib\nfrom pdpbox import pdp\n\nplt.style.use('ggplot')\n\ndata = pd.read_csv('../input/avocado.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Feature engineering and data cleaning\n==============================\nThis dataset is already fairly good, so we do not really have to do any cleaning. Often you will be working with a lot more messy datasets and multiple datasets that have to be merged. We will still have to do a litte feature engineering to get more out of our dataset.\n\n"},{"metadata":{"trusted":true,"_uuid":"4a5a3a91f4a0e74080b6906413e4131213b89b8a"},"cell_type":"code","source":"data['Date'] = pd.to_datetime(data.Date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8207221cfde1485d4886ebaf5eb5f1ede4a4a12c"},"cell_type":"code","source":"data['day_of_week'] = data['Date'].dt.weekday_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"491555997757efe23dfcdb3807b1eac0f169e795"},"cell_type":"code","source":"data.day_of_week.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cd0dc5ab686038c58b64cd9dfd667ef583d71a8"},"cell_type":"code","source":"data['month'] = data['Date'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a588fdaf8b48595d11e20d80344764347ae7e54c"},"cell_type":"code","source":"data['day'] = data['Date'].dt.day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81e5ee51cef6dbb14e5c31d7ad7cd1ace968d3bd"},"cell_type":"code","source":"data = data.rename(columns={'Unnamed: 0': 'Store'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c194f72cd29f01d1ea8b6efec0bfc705002de702"},"cell_type":"markdown","source":"Understanding the dataset\n====================\n\nUnique values of categorical data. It can be useful to know how many unique values you have in your text data, for picking the way of making your categorical data into numrical data."},{"metadata":{"_uuid":"cabd3506f95acbfc7763fcf0697c2297f435d581"},"cell_type":"markdown","source":"<h2>About this Dataset</h2>\n<h4>Context</h4>\n\nIt is a well known fact that Millenials LOVE Avocado Toast. It's also a well known fact that all Millenials live in their parents basements.\n\nClearly, they aren't buying home because they are buying too much Avocado Toast!\n\nBut maybe there's hope... if a Millenial could find a city with cheap avocados, they could live out the Millenial American Dream.\n\nContent\n\nThis data was downloaded from the Hass Avocado Board website in May of 2018 & compiled into a single CSV. Here's how the Hass Avocado Board describes the data on their website:\n\n> The table below represents weekly 2018 retail scan data for National retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. Starting in 2013, the table below reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. The Product Lookup codes (PLU’s) in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this table.\n\n<h4>Some relevant columns in the dataset:</h4>\n\nDate - The date of the observation\nAveragePrice - the average price of a single avocado\ntype - conventional or organic\nyear - the year\nRegion - the city or region of the observation\n\nNumerical column names refer to price lookup codes.\n\n4046:  small Hass\n\n4225:  large Hass\n\n4770:  extra large Hass"},{"metadata":{"_uuid":"6a606e1a543a72ee944a041a34103e43b1b77d24"},"cell_type":"markdown","source":"Therefore for easier reading of our dataset, we can rename the columns with lookup codes as names."},{"metadata":{"trusted":true,"_uuid":"ea31bae12c53c26227f6b1d524c3042c3cbfac15"},"cell_type":"code","source":"data = data.rename(columns={'4046': 'small Hass', '4225':  'large Hass', '4770':  'extra large Hass'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f39783c6f8bc070e8b7aa0f644487bcf9277d7b"},"cell_type":"code","source":"print('Unique values in columns with text:\\n\\n Dates: {0} \\n\\n Data type: {1} \\n\\n Year: {2} \\n\\n Region: {3} \\n\\n Day of week: {4} \\n\\n Month: {5}'.format(data.Date.unique(), data.type.unique(), data.year.unique(), data.region.unique(), data['day_of_week'].unique(), data.month.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f0fc17af62dd9db57377bd7856615af8bfa70c9"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37a02cfd29f304791da8de522e3880d54a4a4cc4"},"cell_type":"markdown","source":"Handeling categorical data\n=====================\n\nI am choosing to handle categorical data, by replacing "},{"metadata":{"trusted":true,"_uuid":"59e34cb6f65837d14a765c24bcd28b878acfa37f"},"cell_type":"code","source":"mappings_type = {'conventional':0, 'organic':1}\n\nmappings_dayofweek = {'Sunday':1}\n\nmappings_region = {}\n\nv = 0\n\nregions = list(data.region.unique())\n\nnumbers = []\n\nfor i in regions:\n    v = v+1\n    numbers.append(v)\n\nd = zip(regions, numbers)\n\nmappings_region = dict(d)\n\ndata.type.replace(mappings_type, inplace=True)\ndata.day_of_week.replace(mappings_dayofweek, inplace=True)\ndata.region.replace(mappings_region, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76c84f844c3805aeb0dde205439c5bfd624f396a"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6959b28bcc75498c02284462f3763b7f4613be76"},"cell_type":"markdown","source":"<h2>Descriptive statistics</h2>\nDescribtive statistics is crusial to understand your dataset properly. Descriptive statistics is the first step to perform good prescriptive and predictive statistics."},{"metadata":{"trusted":true,"_uuid":"077340ef432bf5abbc021d7a51c3487ace9ed7cc"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad9c450a72b5319e77dc134d2c5e56a348531810"},"cell_type":"code","source":"skew_df = pd.DataFrame(data.skew(), columns={'Skewness'})\nskew_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bfe362bc76bdd6fd9d2098bf945cb8ba1c834d9"},"cell_type":"code","source":"kurt_df = pd.DataFrame(data.kurtosis(), columns={'Kurtosis'})\nkurt_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10349db587fa72a8f39ad69eb6fb40132b30019c"},"cell_type":"markdown","source":"<h2>Summarize / What do we know now?</h2>"},{"metadata":{"_uuid":"e1bd2f343d22ce620206c8aaa80743927110a5c6"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"0dbf608beffc509cbe59b62c3d88140ebbccf2e9"},"cell_type":"markdown","source":"Building our model\n===============\n___________\nNow it is finnaly time for some fun! We will be building a XGBoost model and plotting feature importance of it, to learn some more about our data."},{"metadata":{"trusted":true,"_uuid":"f071a45de414e4bd5619680cfd62c417ebb7470b","scrolled":true},"cell_type":"code","source":"X = data.drop(['AveragePrice', 'Date'], axis=1)\n\ny = data['AveragePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nmodel = XGBRegressor(n_jobs=4)\nmodel.fit(X_train, \n            y_train,\n            verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abb21a2bcd368165565faefe6ed4784415f54eeb"},"cell_type":"code","source":"predictions = model.predict(X_test)\nscores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=5)\nprint(scores)\nprint('Mean Absolute Error: %2f' %(-1 * scores.mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43c3c6acc548dd58901bfc5ec8eea95ad904c21a"},"cell_type":"code","source":"mae = mean_absolute_error(predictions, y_test)\nprint(\"Mean Absolute Error : \" + str(mae))\n\nerror_percent = mae/data['AveragePrice'].mean()*100\nprint(str(error_percent) + ' %')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52034f5a820fc6c956a229206a299c9ae7b63957"},"cell_type":"markdown","source":"Feature Selection\n=============\nSelecting the right amount of features, and selecting the right features is crucial to avoid data leakage, overfitting or overfitting.\n\nTherefore we can now do feature importance plot."},{"metadata":{"trusted":true,"_uuid":"d1a04fd6f6db8b841cf0b123c1d975119fa00f05","scrolled":true},"cell_type":"code","source":"# plot feature importance\nfig, ax = plt.subplots(figsize=(15, 15))\nimp_plt = plot_importance(model, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b6b17b9b41eba70def2113b7ce4a26af1d9e5f0"},"cell_type":"code","source":"features_to_plot = ['region', 'year']\ninter1  =  pdp.pdp_interact(model=model, dataset=X_test, model_features=X.columns.tolist(), features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=model, dataset=X_test, model_features=X.columns.tolist(), feature='month')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Month')\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=model, dataset=X_test, model_features=X.columns.tolist(), feature='year')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Year')\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=model, dataset=X_test, model_features=X.columns.tolist(), feature='region')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Region')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"288b06b775af08715ff1d9123f15bcaf3331b86b"},"cell_type":"code","source":"row_to_show = 5\ndata_for_prediction = X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f78656a93b24fcb82560703ea13883c1fd1c8ae"},"cell_type":"code","source":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6557add378d3ebe23223e290e21dedbce8c12ffc"},"cell_type":"markdown","source":"We now know how the model ranks the features, meaning which is the most important features.\n\nNow performing feature selection with scikit learn, can give us a better insight, in how many features might be the optimal number of features to include."},{"metadata":{"trusted":true,"_uuid":"c0b83e8c931e1fd6014a52d72a104e8c5fc74750"},"cell_type":"code","source":"mae = mean_absolute_error(predictions, y_test)\nerror_percent = mae/data['AveragePrice'].mean()*100\n\naccuracy = mean_absolute_error(predictions, y_test)\nprint(\"Mean Absolute Error : \" + str(mae) + \"\\t\" + str(error_percent) + ' %')\n\n#scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=5)\n#print(scores)\n#print('Mean Absolute Error: %2f' %(-1 * scores.mean()))\n# Fit model using each importance as a threshold\nthresholds = sort(model.feature_importances_)\n\nbest_score = {}\n\nfor thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model = XGBRegressor(n_jobs=4)\n    selection_model.fit(select_X_train, y_train)\n    # eval model\n    select_X_test = selection.transform(X_test)\n    predictions = selection_model.predict(select_X_test)\n    #print('Mean Absolute Error: %2f' %(-1 * scores.mean()))\n    accuracy = mean_absolute_error(predictions, y_test)\n    mae = mean_absolute_error(predictions, y_test)\n    error_percent = mae/data['AveragePrice'].mean()*100\n    #print(\"Thresh={0:f}, n={1:f}, Accuracy: {2:f}, Mean Absolute Error {3:f}: , err_perct: {4:f}%\".format(thresh, select_X_train.shape[1], accuracy * 100, mae, error_percent))\n    \n    best_score[select_X_train.shape[1]] = str(error_percent) + ' %'\n\n    \nprint(best_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b6a476c18e22b5c2e6a01807532abb74b2c3896"},"cell_type":"code","source":"value = best_score.values()\nkey = best_score.keys()\n\nmin_val = min(value)\nmin_key = min(best_score, key=best_score.get)\nprint('Best amout of features: key: {0}, value: {1}'.format(min_key, min_val))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ff8fcf3ea67618b39ff528b82583d3ca92814d0"},"cell_type":"markdown","source":"We can now see that the best amount of features corresponding to the feature selection we just did, is 13. We do know which features is the most important from our feature importance plot, this means we can include top 13 features, for building a more optimized model.\n______________________"},{"metadata":{"_uuid":"9f325b361c06f218bb09d68c121c2cd9bccbb031"},"cell_type":"markdown","source":"Building a better model\n==================\nNow that we know which features is the most important ones, and we know which might possible be the best number of features to include. We can build an optimized model, and let it find the best hyper parameters using cross-validation.\n\n--------\nWe start by selscting the data we need."},{"metadata":{"trusted":true,"_uuid":"6f7a51da18983b85d4094cb154ba7db0d5ded9c0"},"cell_type":"code","source":"X_opt = data.drop(['AveragePrice', 'Date', 'XLarge Bags'], axis=1)\ny_opt = data['AveragePrice']\n\nopt_X_train, opt_X_test, opt_y_train, opt_y_test = train_test_split(X_opt, y_opt, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d9611dfbcfa561b62c441d011a203b30e7eb85a"},"cell_type":"markdown","source":"Now that we have the data, we can build our pipeline, using gridsearch cv to find the best hyperparameters."},{"metadata":{"trusted":true,"_uuid":"47eb1a31b26d99db566adffaaa24bd1000022577"},"cell_type":"code","source":"opt_pipeline = Pipeline([('xgb', XGBRegressor(n_jobs=4))])\n\nparam_grid = {\n    \"xgb__n_estimators\": [100, 250, 500, 1000],\n    \"xgb__learning_rate\": [0.1, 0.25, 0.5, 1],\n    \"xgb__max_depth\": [6, 7, 8],\n    \"xgb__min_child_weight\": [0.25, 0.5, 1, 1.5]\n}\n\nfit_params = {\"xgb__eval_set\": [(opt_X_test, opt_y_test)], \n              \"xgb__early_stopping_rounds\": 10, \n              \"xgb__verbose\": False} \n\nsearchCV = GridSearchCV(opt_pipeline, cv=5, param_grid=param_grid, fit_params=fit_params)\nsearchCV.fit(opt_X_train, opt_y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c87f52b26671826e1220e6ab0f88d33c45baa77a"},"cell_type":"code","source":"searchCV.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2066ad5e016de2061befcdccbbb09a84492dfb8"},"cell_type":"code","source":"searchCV.cv_results_['mean_train_score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b8ba7a7bfd4a054f1cf2df3798d46dbf22854d7"},"cell_type":"code","source":"searchCV.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5facbc97219cb33ded499e19805be47ab4b6029f"},"cell_type":"code","source":"searchCV.cv_results_['mean_train_score'].mean(), searchCV.cv_results_['mean_test_score'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b194ba766ee1e180fc548a31898c7cb7a9d17fb4"},"cell_type":"code","source":"opt_predictions = searchCV.predict(opt_X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27736e7e6a89415320b8e93d047dda580eb48ee3"},"cell_type":"code","source":"mae = mean_absolute_error(opt_predictions, opt_y_test)\nprint(\"Mean Absolute Error : \" + str(mae))\n\nerror_percent = mae/data['AveragePrice'].mean()*100\nprint(\"Error percentage: \" + str(error_percent) + ' %')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4823f2d483dccc0da2a404ecac40a5eec8ad098d"},"cell_type":"markdown","source":"As we can see We have actually improved our model quite a bit, just with this basic optimazation and feature selection"},{"metadata":{"_uuid":"7467f3930a254f97005bf994ab1d5c63baaf5525"},"cell_type":"markdown","source":"features_to_plot = ['region', 'year']\ninter1  =  pdp.pdp_interact(model=searchCV, dataset=opt_X_test, model_features=X_opt.columns.tolist(), features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')"},{"metadata":{"_uuid":"de54177698ebe5520c092b8d550eed774dc1cf76"},"cell_type":"markdown","source":"Futher Usage Of The Model\n=====================\n-----------------\nWe can now use our model in other applications, by saving it and loading it in another python program. You can also build an API with it and the include it in an app, on a website, a bigger production system etc.\n\n\nLets go ahead and save the model."},{"metadata":{"trusted":true,"_uuid":"e02d798b1b7ea06facb89583ebf79790ad8436b1"},"cell_type":"code","source":"joblib.dump(searchCV, \"xgboostmodel.joblib.dat\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7149ebf9605459aabfb3acb54aa68fa850f957f"},"cell_type":"markdown","source":"For further information on how to load it into another application, lookup saving Gradient Boosting Models with joblib."},{"metadata":{"_uuid":"553baff85d82eaf3e310320fa096b7558108f7b4"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}